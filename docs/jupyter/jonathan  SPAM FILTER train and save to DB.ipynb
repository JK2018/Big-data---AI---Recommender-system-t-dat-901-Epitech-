{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a634d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import walk\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "#import pprint \n",
    "#from jange import ops, stream, vis\n",
    "import langdetect\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.metrics import silhouette_score\n",
    "#from sklearn.cluster import MiniBatchKMeans\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from random import randrange\n",
    "#import unidecode\n",
    "#import csv\n",
    "import pymongo\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "DATASET_PATH = '/Users/jonathankhalifa/Desktop/T-AIA-901/BOOTSTRAP/discours/tous'\n",
    "STOPS_DATASET_PATH = '/Users/jonathankhalifa/Desktop/T-AIA-901/PROJECT/data_sncf/stops.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892eaf3",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "## Dataset creation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13b4f4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef remove_cities(message, stopNamesList2):\\n    doc = nlp(message) #lower\\n    \\n    def saveAllCitiesInArray():\\n        cities = []\\n        for city in doc.ents:\\n            cities.append(city.text)\\n        return cities\\n    cityArr = saveAllCitiesInArray()\\n    \\n    \\n    def checkCity(city):\\n        city = city.lower()\\n        city = city.replace(\"-\", \" \")\\n        city = city.replace(\"saint\", \"st\")\\n\\n        for index, row in stopNamesList2.iterrows():\\n            processedStopName = row[\\'stop_name\\'].replace(\"-\", \" \").lower()\\n            if (city in processedStopName):\\n                message = message.replace(city, \"\")\\n                break\\n        return message\\n    \\n    \\n    for c in cityArr:\\n        message = checkCity(c)\\n        \\n        \\n    #return (cityArr)\\n    return (messsage)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = pymongo.MongoClient(\"mongodb+srv://admin:admin@clusteria.tvj6u.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "db = client['iadb']\n",
    "collection3 = db['stopNames']\n",
    "collection4 = db['graphSegments']\n",
    "\n",
    "cursor = collection3.find({})   \n",
    "fields = ['stop_name']\n",
    "stopNamesList2 = pd.DataFrame(list(cursor), columns = fields)\n",
    "stopNamesList2['stop_name']= stopNamesList2['stop_name'].str.replace(\"Gare de \",\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_initial_dataset(path):\n",
    "    \"\"\"\n",
    "    IN : path to raw text dataset on hard drive\n",
    "    OUT : dataframe\n",
    "    \"\"\"\n",
    "    discours_path = path\n",
    "    dataset = {}\n",
    "    fullDataset = {}\n",
    "    dset = []\n",
    "    filenames = next(walk(discours_path), (None, None, {}))[2] \n",
    "\n",
    "    for f in filenames:\n",
    "        ff = open(discours_path + '/' + f, 'r')\n",
    "        file_contents = ff.read()\n",
    "\n",
    "        file_contents = file_contents.replace(\"\\n\", \" \")\n",
    "\n",
    "        x = file_contents.split('.')\n",
    "        for xx in x:\n",
    "            dset.append(xx)\n",
    "        ff.close()\n",
    "    df = pd.DataFrame(dset)\n",
    "    df.rename(columns={0: 'text_input'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def inject_stop_name(phrase):\n",
    "    \"\"\"\n",
    "    IN : 1 text row from initial dataset\n",
    "    OUT : text with a random stop name injected\n",
    "    USE : inject a random stop name at a random position in the text\n",
    "    \"\"\"\n",
    "    split_strings = phrase.split()\n",
    "    inj_pos = len(split_strings)-1\n",
    "    inj_pos = randrange(inj_pos)\n",
    "\n",
    "    rand_stop = len(stopNamesList2['stop_name'])-1\n",
    "    rand_stop = randrange(rand_stop)\n",
    "    rand_stop = stopNamesList2.at[rand_stop,'stop_name']\n",
    "\n",
    "    split_strings.insert(inj_pos, str(rand_stop))\n",
    "    final_string = ' '.join(split_strings)\n",
    "    return final_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inject_good_words(phrase):\n",
    "    \"\"\"\n",
    "    IN : 1 text row from initial dataset\n",
    "    OUT : text with a random ham/good word injected\n",
    "    USE : inject a random ham related word at a random position in the text\n",
    "    \"\"\"\n",
    "    #good_words = ['voudrait','voudrais','souhaite' ,'veux', 'billet', 'prix', 'train', 'acheter', 'départ', 'arrivée', 'destination', 'réserver', 'aller', 'retour', 'aller-retour', 'partir', 'aller']\n",
    "    good_words = ['billet','train','aller', 'retour', 'vouloir', 'souhaiter', 'acheter','billet', 'départ','retour']\n",
    "    \n",
    "    split_strings = phrase.split()\n",
    "    inj_pos = len(split_strings)-1\n",
    "    inj_pos = randrange(inj_pos)\n",
    "\n",
    "    rand_word = len(good_words)-1\n",
    "    rand_word = randrange(rand_word)\n",
    "    rand_word = good_words[rand_word]\n",
    "\n",
    "    split_strings.insert(inj_pos, rand_word)\n",
    "    final_string = ' '.join(split_strings)\n",
    "    return final_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inject_spam_words(phrase):\n",
    "    \"\"\"\n",
    "    IN : 1 text row from initial dataset\n",
    "    OUT : text with a random spam word injected\n",
    "    USE : inject a random spam related word at a random position in the text\n",
    "    \"\"\"\n",
    "    spam_words = ['avion', 'vol', 'aeroport', 'port', 'routière', 'autoroute', 'bus', 'autocar',\n",
    "                  'autobus', 'remboursement', 'rembourser', 'bateau', 'voiture', 'pied', 'marcher', 'concert',\n",
    "                 'dinner', 'spectacle']\n",
    "    \n",
    "    split_strings = phrase.split()\n",
    "    \n",
    "    inj_pos = len(split_strings)-1\n",
    "    inj_pos = randrange(inj_pos)\n",
    "\n",
    "    rand_word = len(spam_words)-1\n",
    "    rand_word = randrange(rand_word)\n",
    "    rand_word = spam_words[rand_word]\n",
    "\n",
    "    split_strings.insert(inj_pos, rand_word)\n",
    "    final_string = ' '.join(split_strings)\n",
    "    return final_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_test_val_split(dfnorm, dfspam, coef_ham, coef_spam):\n",
    "    \"\"\"\n",
    "        IN : ham dataset, spam dataset\n",
    "        OUT : train dataset, test dataset, validation dataset\n",
    "        USE : Splits dataset into traditionnal 70/29/1. coef_ham/spam \n",
    "              to set the ratio of ham/spam in each split ex: 0.8 for 80%\n",
    "    \"\"\"\n",
    "    l_validation = 10\n",
    "    l_train = round((len(dfnorm)+len(dfspam))*0.7)-10\n",
    "    l_test = round((len(dfnorm)+len(dfspam))*0.3)\n",
    "    \n",
    "    val1 = dfnorm.iloc[ :int(l_validation*coef_ham)]\n",
    "    val2 = dfspam.iloc[ :int(l_validation*coef_spam)]\n",
    "    train1 =  dfnorm.iloc[int(l_validation*coef_ham):int(l_train*coef_ham)]\n",
    "    train2 =  dfspam.iloc[int(l_validation*coef_spam):int(l_train*coef_spam)]\n",
    "    test1 =  dfnorm.iloc[int(l_validation*coef_ham)+int(l_train*coef_ham):]\n",
    "    test2 =  dfspam.iloc[int(l_validation*coef_spam)+int(l_train*coef_spam):]\n",
    "    \n",
    "    # concat spam and norm\n",
    "    df_train = pd.concat([train1, train2])\n",
    "    df_test = pd.concat([test1, test2])\n",
    "    df_val = pd.concat([val1, val2])\n",
    "    \n",
    "    # shuffle rows\n",
    "    df_train = shuffle(df_train)\n",
    "    df_test = shuffle(df_test)\n",
    "    df_val = shuffle(df_val)\n",
    "    \n",
    "    #reset indexes\n",
    "    df_val.reset_index(drop=True, inplace=True)\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    result = [df_train, df_test, df_val]\n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "def detect_lang(text):\n",
    "    result = langdetect.detect_langs(text)\n",
    "    lang = str(result[0])[:2]\n",
    "    return lang\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def remove_cities_s(message):\n",
    "    doc = nlp(message) #lower\n",
    "    for city in doc.ents:\n",
    "        #print(city)\n",
    "        message = message.replace(str(city), \"\")\n",
    "        #print(message)\n",
    "    return (message)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def remove_cities(message, stopNamesList2):\n",
    "    doc = nlp(message) #lower\n",
    "    \n",
    "    def saveAllCitiesInArray():\n",
    "        cities = []\n",
    "        for city in doc.ents:\n",
    "            cities.append(city.text)\n",
    "        return cities\n",
    "    cityArr = saveAllCitiesInArray()\n",
    "    \n",
    "    \n",
    "    def checkCity(city):\n",
    "        city = city.lower()\n",
    "        city = city.replace(\"-\", \" \")\n",
    "        city = city.replace(\"saint\", \"st\")\n",
    "\n",
    "        for index, row in stopNamesList2.iterrows():\n",
    "            processedStopName = row['stop_name'].replace(\"-\", \" \").lower()\n",
    "            if (city in processedStopName):\n",
    "                message = message.replace(city, \"\")\n",
    "                break\n",
    "        return message\n",
    "    \n",
    "    \n",
    "    for c in cityArr:\n",
    "        message = checkCity(c)\n",
    "        \n",
    "        \n",
    "    #return (cityArr)\n",
    "    return (messsage)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000cdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3eb6b27",
   "metadata": {},
   "source": [
    "#### To generate our dataset we will do the following : \n",
    "    \n",
    "    Remove all texts under 100 chars long\n",
    "    Split the initial dataset in half, 50% will become spam and 50% ham\n",
    "    For spam we will : inject one random bad word and one stop_name\n",
    "    For ham we will inject two stop_names and one good word\n",
    "    We label the dataset\n",
    "    We split in train test val with a 70-29-1 ratio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29ad9fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inject 2 cities in all phrases\n",
    "# keep the phrases that spacy detects both cities\n",
    "# then split in half and add bad words etc\n",
    "#\n",
    "#\n",
    "\n",
    "def generate_labeled_dataset(df):\n",
    "    \"\"\"\n",
    "    IN : initial text dataset\n",
    "    OUT : 3 dataframes (train 70%, test 30%, val 10rows) \n",
    "    USE : Used to create the final dataset that will be used for the spam \n",
    "          filter training/testing. Dataset is splitted, labelled norm or spam\n",
    "          and injected with words according to label.\n",
    "          Coef_ham/spam to set the ratio of ham/spam ex: 0.8 for 80%\n",
    "    \"\"\"\n",
    "    \n",
    "    coef_ham = 0.5\n",
    "    coef_spam = 0.5\n",
    "    \n",
    "    \n",
    "    # remove texts with under 100 chars in length\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row['text_input']) < 100:\n",
    "            df = df.drop([index])\n",
    "            \n",
    "            \n",
    "    df['text_input'] = df['text_input'].transform(remove_cities_s)\n",
    "    \n",
    "    \n",
    "            \n",
    "    \"\"\"        \n",
    "    df['text_input'] = df['text_input'].transform(inject_stop_name)\n",
    "    df['text_input'] = df['text_input'].transform(inject_stop_name)\n",
    "    print(len(df))\n",
    "    \n",
    "    indArr=[]\n",
    "    for index, row in df.iterrows():\n",
    "        aa = check_two_cities(row['text_input'], stopNamesList2)\n",
    "        if(aa != 2):\n",
    "            indArr.append(index)\n",
    "            \n",
    "    \n",
    "    print(indArr)\n",
    "    df = df.drop(indArr)\n",
    "    print(len(df))\n",
    "    \"\"\"\n",
    "    \n",
    "    # split in half\n",
    "    a = round(len(df)*coef_ham)\n",
    "    dfnorm = df.iloc[ :a+5]\n",
    "    dfspam = df.iloc[ a-5:]\n",
    "    \n",
    "    dfnorm['text_input'] = dfnorm['text_input'].transform(inject_good_words)\n",
    "    dfnorm['text_input'] = dfnorm['text_input'].transform(inject_good_words)\n",
    "    dfnorm['text_label'] = \"ham\"\n",
    "    \n",
    "    #inj bad words\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    dfspam['text_label'] = \"spam\"\n",
    "    \n",
    "    result = train_test_val_split(dfnorm, dfspam, coef_ham, coef_spam)\n",
    "    \n",
    "############\n",
    "    \n",
    "    #split main df in half\n",
    "    #a = round(len(df)*coef_ham)\n",
    "    #dfnorm = df.iloc[ :a]\n",
    "    #dfspam = df.iloc[ a:]\n",
    "\n",
    "    #dfStops_processed = load_clean_cities()\n",
    "\n",
    "    # we add two stops and 3 good word and set label as ham\n",
    "    #dfnorm['text_input'] = dfnorm['text_input'].transform(inject_good_words)\n",
    "    #dfnorm['text_input'] = dfnorm['text_input'].transform(inject_good_words)\n",
    "    #dfnorm['text_input'] = dfnorm['text_input'].transform(inject_good_words)\n",
    "  #  dfnorm['text_input'] = dfnorm['text_input'].transform(inject_stop_name)\n",
    "  #  dfnorm['text_input'] = dfnorm['text_input'].transform(inject_stop_name)\n",
    "  #  dfnorm['text_label'] = \"ham\"\n",
    "\n",
    "    \n",
    "    # add 4 bad word on all rows , 2 stop name on 100% of the rows , and set label as spam\n",
    "  #  dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "  #  dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "  #  dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "  #  dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "  #  dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "  #  dfspam['text_input'] = dfspam['text_input'].transform(inject_spam_words)\n",
    "    \n",
    "  #  dfspam['text_input'] = dfspam['text_input'].transform(inject_stop_name)\n",
    "  #  p = round(len(dfspam)/2)\n",
    "  #  dfspam['text_input'][:p] = dfspam['text_input'][:p].transform(inject_stop_name)\n",
    "  #  dfspam['text_label'] = \"spam\"\n",
    "    \n",
    "    \n",
    "    #define train test and validation split\n",
    "  #  result = train_test_val_split(dfnorm, dfspam, coef_ham, coef_spam)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048bad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292d31d7",
   "metadata": {},
   "source": [
    "#### As result of our dataset generation, we obtain a fully labelled and splited dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e7cc375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_initial_dataset(DATASET_PATH)\n",
    "\n",
    "#dfStops_processed = load_clean_cities()\n",
    "\n",
    "df_train, df_test, df_validate = generate_labeled_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ac79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9017e8f8",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "## Preprocessing user input\n",
    "\n",
    "#### In this phase, we will do the following to each text :\n",
    "\n",
    "    set to lowercase\n",
    "    remove punctuation\n",
    "    remove special chars\n",
    "    remove stop words\n",
    "    remove extra spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4def7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess for before vectorizing/training\n",
    "\n",
    "def preprocess_string(string):\n",
    "    \"\"\"\n",
    "    IN : user input\n",
    "    OUT : cleaned user input\n",
    "    USE : will set all to lowercase, remove punctuation and stopwords,\n",
    "          remove trailing and double spaces\n",
    "    \"\"\"\n",
    "    # set all to lowercase\n",
    "    string = string.lower()\n",
    "    # remove punct\n",
    "    string = string.replace('[^\\w\\s]',' ')\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('french')\n",
    "    string = ' '.join([word for word in string.split(\" \") if word not in stopwords.words('french')])\n",
    "    # replace double space by single space\n",
    "    string = string.replace('  ',' ')\n",
    "    # strip spaces\n",
    "    string = string.strip()\n",
    "    return string\n",
    "\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    \"\"\"\n",
    "    IN : df of user inputs\n",
    "    OUT : cleaned df of user inputs\n",
    "    USE : will set all to lowercase, remove punctuation and stopwords,\n",
    "          remove trailing and double spaces\n",
    "    \"\"\"\n",
    "    df['text_input'] = df['text_input'].str.lower()\n",
    "    df['text_input'] = df['text_input'].str.replace('[^\\w\\s]',' ')\n",
    "    stop = stopwords.words('french')\n",
    "    df['text_input'] = df['text_input'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    df['text_input'] = df['text_input'].str.replace('  ',' ')\n",
    "    df['text_input'] = df['text_input'].str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422ce90",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "## Trainning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3b6883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocab(df_train):\n",
    "    \"\"\"\n",
    "    IN : train datasset\n",
    "    OUT : list of each unique word in the dataset\n",
    "    \"\"\"\n",
    "    df_train['text_input'] = df_train['text_input'].str.split()\n",
    "\n",
    "    vocabulary = []\n",
    "    for text in df_train['text_input']:\n",
    "       for word in text:\n",
    "          vocabulary.append(word)\n",
    "\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_frequency(vocabulary, df_train):\n",
    "    \"\"\"\n",
    "    IN : train dataset, vocab list\n",
    "    OUT : dataset with word frequency matrix\n",
    "    \"\"\"\n",
    "    word_counts_per_text = {unique_word: [0] * len(df_train['text_input']) for unique_word in vocabulary}\n",
    "\n",
    "    for index, text in enumerate(df_train['text_input']):\n",
    "       for word in text:\n",
    "          word_counts_per_text[word][index] += 1\n",
    "\n",
    "    word_counts = pd.DataFrame(word_counts_per_text)\n",
    "    training_set_clean = pd.concat([df_train, word_counts], axis=1)\n",
    "    return training_set_clean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_constants(training_set_clean, vocabulary):\n",
    "    \"\"\"\n",
    "    IN : train word frequency dataset , vocab list\n",
    "    OUT : list of model constants\n",
    "    \"\"\"\n",
    "    # Isolating spam and ham messages first\n",
    "    spam_messages = training_set_clean[training_set_clean['text_label'] == 'spam']\n",
    "    ham_messages = training_set_clean[training_set_clean['text_label'] == 'ham']\n",
    "\n",
    "    # P(Spam) and P(Ham)\n",
    "    p_spam = len(spam_messages) / len(training_set_clean)\n",
    "    p_ham = len(ham_messages) / len(training_set_clean)\n",
    "\n",
    "    # N_Spam\n",
    "    n_words_per_spam_message = spam_messages['text_input'].apply(len)\n",
    "    n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "    # N_Ham\n",
    "    n_words_per_ham_message = ham_messages['text_input'].apply(len)\n",
    "    n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "    # N_Vocabulary\n",
    "    n_vocabulary = len(vocabulary)\n",
    "\n",
    "    # Laplace smoothing\n",
    "    alpha = 1\n",
    "    return [spam_messages,ham_messages, p_spam, p_ham, n_words_per_spam_message, n_spam, n_words_per_ham_message, n_ham, n_vocabulary, alpha]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(df_train):\n",
    "    \"\"\"\n",
    "    IN : train dataset\n",
    "    OUT : list of model params\n",
    "    USE : Func used to fully train the model and retrieve the weights/params\n",
    "          To be runned once.\n",
    "    \"\"\"\n",
    "    \n",
    "    # we preprocess/clean each dataset\n",
    "    df_train = preprocess_df(df_train)\n",
    "    \n",
    "    # we extract df_train s vocabulary\n",
    "    vocabulary = extract_vocab(df_train)\n",
    "    \n",
    "    # we count the freq of each word from vocabulary in df_train\n",
    "    training_set_clean = word_frequency(vocabulary, df_train)\n",
    "    \n",
    "    # we calculate our constants\n",
    "    spam_messages, ham_messages, p_spam, p_ham, n_words_per_spam_message, n_spam, n_words_per_ham_message, n_ham, n_vocabulary, alpha = calc_constants(training_set_clean, vocabulary)\n",
    "    \n",
    "    # we calculate our parameters\n",
    "    # Initiate parameters\n",
    "    parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "    parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "    # Calculate parameters\n",
    "    for word in vocabulary:\n",
    "        n_word_given_spam = spam_messages[word].sum() # spam_messages already defined\n",
    "        p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "        parameters_spam[word] = p_word_given_spam\n",
    "        \n",
    "        #print(ham_messages[word].sum())\n",
    "        n_word_given_ham = ham_messages[word].sum() # ham_messages already defined\n",
    "        p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "        parameters_ham[word] = p_word_given_ham\n",
    "    return [p_word_given_ham, p_word_given_spam, parameters_spam, parameters_ham, p_spam, p_ham]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99994667",
   "metadata": {},
   "source": [
    "#### As result of training we get the params of our model (the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ec934f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_word_given_ham, p_word_given_spam, parameters_spam, parameters_ham, p_spam, p_ham = train_model(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "546de0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAUNCH ONLY ONCE!\n",
    "\n",
    "# inssert items in mongo atlas DB\n",
    "client = pymongo.MongoClient(\"mongodb+srv://admin:admin@clusteria.tvj6u.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "db = client['iadb']\n",
    "dataset = db['spamfilterParams']\n",
    "dataset2 = db['cities']\n",
    "\n",
    "import json\n",
    "# to load data into mongodb\n",
    "a = {'_id': 'p_word_given_ham', 'data': p_word_given_ham}   \n",
    "b = {'_id': 'p_word_given_spam', 'data': p_word_given_spam}\n",
    "c = {'_id': 'parameters_spam', 'data': parameters_spam}\n",
    "d = {'_id': 'parameters_ham', 'data': parameters_ham}\n",
    "e = {'_id': 'p_ham', 'data': p_ham}\n",
    "f = {'_id': 'p_spam', 'data': p_spam}\n",
    "\n",
    "\n",
    "# to load city data into mongodb\n",
    "stopNamesList2.index = stopNamesList2.index.map(str)\n",
    "g=[]\n",
    "for index, row in stopNamesList2.iterrows():\n",
    "    g.append(row.to_dict())  \n",
    "#x = dataset2.insert_many(g)\n",
    "\n",
    "x = dataset.insert_many([a,b,c,d,e,f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36079b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd2fb5b4",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "## Test phase\n",
    "#### We run our filter on our test dataset and determine the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc7133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "802ca2c1",
   "metadata": {},
   "source": [
    "#### We determine the model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0ddd254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef model_accuracy(df_test2):\\n\\n    correct = 0\\n    total = df_test2.shape[0]\\n\\n    for row in df_test2.iterrows():\\n       row = row[1]\\n       if row['text_label'] == row['predicted']:\\n          correct += 1\\n    return {'Correct':correct, 'Incorrect':total - correct, 'Accuracy': correct/total}\\n\\n\\n\\nprint(model_accuracy(df_test2))\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def model_accuracy(df_test2):\n",
    "\n",
    "    correct = 0\n",
    "    total = df_test2.shape[0]\n",
    "\n",
    "    for row in df_test2.iterrows():\n",
    "       row = row[1]\n",
    "       if row['text_label'] == row['predicted']:\n",
    "          correct += 1\n",
    "    return {'Correct':correct, 'Incorrect':total - correct, 'Accuracy': correct/total}\n",
    "\n",
    "\n",
    "\n",
    "print(model_accuracy(df_test2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ccd75",
   "metadata": {},
   "source": [
    "\n",
    "<br><br><br><br>\n",
    "## Prediction examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebeef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a0bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05fa62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a93ccd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1368364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "334a0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853eafa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
