{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "471fe467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pymongo\n",
    "import langdetect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "874c1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET DATA FROM DB\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://admin:admin@clusteria.tvj6u.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "db = client['iadb']\n",
    "collection = db['spamfilterParams']\n",
    "collection2 = db['cities']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p_word_given_ham = collection.find_one({'_id': \"p_word_given_ham\" })\n",
    "p_word_given_spam = collection.find_one({'_id': \"p_word_given_spam\" })\n",
    "parameters_spam = collection.find_one({'_id': \"parameters_spam\" })\n",
    "parameters_ham = collection.find_one({'_id': \"parameters_ham\" })\n",
    "p_ham = collection.find_one({'_id': \"p_ham\" })\n",
    "p_spam = collection.find_one({'_id': \"p_spam\" })\n",
    "\n",
    "\n",
    "dfStops_processedCursor = collection2.find({})   \n",
    "fields = ['stop_name']\n",
    "dfStops_processed = pd.DataFrame(list(dfStops_processedCursor), columns = fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e497af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62a94e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################### REMOVE ONCE CITY LIST IS IN DB\n",
    "\"\"\"\n",
    "stops = pd.read_csv('/Users/jonathankhalifa/Desktop/T-AIA-901/PROJECT/data_sncf/stops.txt', sep=\",\", header=0)\n",
    "stops = stops.drop('location_type', 1)\n",
    "stops = stops.drop('stop_url', 1)\n",
    "stops = stops.drop('zone_id', 1)\n",
    "stops = stops.drop('stop_desc', 1)\n",
    "stops = stops.drop('stop_lat', 1)\n",
    "stops = stops.drop('stop_lon', 1)\n",
    "stops['stop_id'] = stops['stop_id'].apply(lambda x: x[-8:])\n",
    "#stops.head()\n",
    "\n",
    "\n",
    "# 3\n",
    "trips= pd.read_csv('/Users/jonathankhalifa/Desktop/T-AIA-901/PROJECT/data_sncf/trips.txt', sep=\",\", header=0)\n",
    "trips = trips.drop('block_id', 1)\n",
    "trips = trips.drop('shape_id', 1)\n",
    "trips = trips.drop('direction_id', 1)\n",
    "trips = trips.drop('service_id', 1)\n",
    "#trips\n",
    "\n",
    "\n",
    "# 4\n",
    "stop_times= pd.read_csv('/Users/jonathankhalifa/Desktop/T-AIA-901/PROJECT/data_sncf/stop_times.txt', sep=\",\", header=0)\n",
    "stop_times = stop_times.drop('shape_dist_traveled', 1)\n",
    "stop_times = stop_times.drop('drop_off_type', 1)\n",
    "stop_times = stop_times.drop('pickup_type', 1)\n",
    "stop_times = stop_times.drop('stop_headsign', 1)\n",
    "# keep 8 last digits from stop id col\n",
    "stop_times['stop_id'] = stop_times['stop_id'].apply(lambda x: x[-8:])\n",
    "#stop_times\n",
    "\n",
    "\n",
    "# 5\n",
    "routes= pd.read_csv('/Users/jonathankhalifa/Desktop/T-AIA-901/PROJECT/data_sncf/routes.txt', sep=\",\", header=0)\n",
    "routes = routes.drop('agency_id', 1)\n",
    "routes = routes.drop('route_desc', 1)\n",
    "routes = routes.drop('route_url', 1)\n",
    "routes = routes.drop('route_color', 1)\n",
    "routes = routes.drop('route_text_color', 1)\n",
    "routes = routes.drop('route_type', 1)\n",
    "#routes.head()\n",
    "\n",
    "\n",
    "# 6\n",
    "#calendar= pd.read_csv('/Users/jonathankhalifa/Desktop/T-AIA-901/PROJECT/data_sncf/calendar.txt', sep=\",\", header=0)\n",
    "#calendar.head()\n",
    "\n",
    "# 7\n",
    "#calendar_dates= pd.read_csv('/Users/jonathankhalifa/Desktop/T-AIA-901/PROJECT/data_sncf/calendar_dates.txt', sep=\",\", header=0)\n",
    "#calendar_dates = calendar_dates.drop('exception_type', 1)\n",
    "#calendar_dates.head()\n",
    "\n",
    "# 8\n",
    "#agency= pd.read_csv('/Users/jonathankhalifa/Desktop/T-AIA-901/PROJECT/data_sncf/agency.txt', sep=\",\", header=0)\n",
    "#agency.head()\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "x = pd.merge(stop_times, trips, on='trip_id', how='outer')\n",
    "x = pd.merge(stops, x, on='stop_id', how='outer')\n",
    "x = x.sort_values(by=['trip_id','stop_sequence'])\n",
    "x = x.drop_duplicates()\n",
    "x = x[x['stop_sequence'].notna()]\n",
    "x = x[~x.drop('parent_station', axis=1).duplicated(keep=False) | pd.notna(x['parent_station'])]\n",
    "x = x.reset_index(drop=True)\n",
    "#x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FETCH from DB\n",
    "\n",
    "stopNamesList = x[['stop_name']].dropna()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://admin:admin@clusteria.tvj6u.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "db = client['iadb']\n",
    "collection = db['cities']\n",
    "cursor = collection.find({})\n",
    "fields = ['stop_name']\n",
    "stopNamesList = pd.DataFrame(list(cursor), columns = fields)\n",
    "\n",
    "\n",
    "def check_two_cities(message, stopNamesList):\n",
    "    doc = nlp(message) #lower\n",
    "    \n",
    "    def saveAllCitiesInArray():\n",
    "        cities = []\n",
    "        for city in doc.ents:\n",
    "            cities.append(city.text)\n",
    "        return cities\n",
    "    cityArr = saveAllCitiesInArray()\n",
    "    \n",
    "    \n",
    "    def checkCity(city):\n",
    "        city = city.lower()\n",
    "        city = city.replace(\"-\", \" \")\n",
    "        city = city.replace(\"saint\", \"st\")\n",
    "        result = 0\n",
    "        for index, row in stopNamesList.iterrows():\n",
    "            processedStopName = row['stop_name'].replace(\"-\", \" \").lower()\n",
    "            if (city in processedStopName):\n",
    "                result = 1\n",
    "                break\n",
    "            else:\n",
    "                result = 0\n",
    "        #print(result)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    nbCitiesConfirmed = 0\n",
    "    for c in cityArr:\n",
    "        nbCitiesConfirmed = nbCitiesConfirmed + checkCity(c)\n",
    "        \n",
    "    #return (cityArr)\n",
    "    return (nbCitiesConfirmed)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# TEST\n",
    "text = (\"Paris en venant de Mexico\")   \n",
    "check_two_cities(text,stopNamesList)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f137224b",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classify(message, p_word_given_ham, p_word_given_spam, parameters_spam, parameters_ham, p_spam, p_ham):\n",
    "     \n",
    "    \"\"\"\n",
    "    IN : model params, user input\n",
    "    OUT : model s prediction (ham / spam)\n",
    "    USE : func that predicts weather a user input is spam or ham by\n",
    "          applying our model params to a Naive Bayes model\n",
    "          also checks for FR lang, and number of cities in the input\n",
    "          Used for predicting user input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect_lang(text):\n",
    "        result = langdetect.detect_langs(text)\n",
    "        lang = str(result[0])[:2]\n",
    "        return lang\n",
    "    \n",
    "    def check_two_cities(message, stopNamesList):\n",
    "        doc = nlp(message) #lower\n",
    "\n",
    "        def saveAllCitiesInArray():\n",
    "            cities = []\n",
    "            for city in doc.ents:\n",
    "                cities.append(city.text)\n",
    "            return cities\n",
    "        cityArr = saveAllCitiesInArray()\n",
    "        #print(cityArr)\n",
    "\n",
    "        def checkCity(city):\n",
    "            city = city.lower()\n",
    "            city = city.replace(\"-\", \" \")\n",
    "            city = city.replace(\"saint\", \"st\")\n",
    "            result = 0\n",
    "            for index, row in stopNamesList.iterrows():\n",
    "                processedStopName = row['stop_name'].replace(\"-\", \" \").lower()\n",
    "                if (city in processedStopName):\n",
    "                    result = 1\n",
    "                    break\n",
    "                else:\n",
    "                    result = 0\n",
    "            #print(result)\n",
    "            return result\n",
    "\n",
    "\n",
    "        nbCitiesConfirmed = 0\n",
    "        for c in cityArr:\n",
    "            nbCitiesConfirmed = nbCitiesConfirmed + checkCity(c)\n",
    "\n",
    "        #print(nbCitiesConfirmed)\n",
    "        return (nbCitiesConfirmed)\n",
    "\n",
    "    \"\"\"\n",
    "    def check_two_cities(message, dfStops_processed):\n",
    "        message_tokenized = message.split(\" \")\n",
    "        counter = 0\n",
    "        for index, row in dfStops_processed.iterrows():\n",
    "            for r in message_tokenized:\n",
    "                if len(str(row['stop_name']))>1 and row['stop_name'] == r:\n",
    "                    counter+=1\n",
    "        return counter\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocess_string(string):\n",
    "        \"\"\"\n",
    "        IN : user input\n",
    "        OUT : cleaned user input\n",
    "        USE : will set all to lowercase, remove punctuation and stopwords,\n",
    "              remove trailing and double spaces\n",
    "        \"\"\"\n",
    "        # set all to lowercase\n",
    "        string = string.lower()\n",
    "        # remove punct\n",
    "        string = string.replace('[^\\w\\s]',' ')\n",
    "        # remove stop words\n",
    "        stop = stopwords.words('french')\n",
    "        string = ' '.join([word for word in string.split(\" \") if word not in stopwords.words('french')])\n",
    "        # replace double space by single space\n",
    "        string = string.replace('  ',' ')\n",
    "        # strip spaces\n",
    "        string = string.strip()\n",
    "        return string\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    result = \"\"\n",
    "    # check cities\n",
    "    nb_of_cities = check_two_cities(message, dfStops_processed)\n",
    "   \n",
    "    # check lang\n",
    "    lang = detect_lang(message)\n",
    "    \n",
    "    message = message.replace(',','')\n",
    "    message = message.replace('-','')\n",
    "    message = message.replace(' -','')\n",
    "    message = message.replace(' /','')\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = preprocess_string(message)\n",
    "    \n",
    "    \n",
    "    message2 = \"\"\n",
    "    doc = nlp(message)\n",
    "    for token in doc:\n",
    "        message2 = message2+ \" \"+token.lemma_\n",
    "\n",
    "    if lang != 'fr' and len(doc) > 3:\n",
    "        result = 'spam'\n",
    "    else:\n",
    "        if nb_of_cities < 2:\n",
    "            result = 'spam'\n",
    "        else:\n",
    "            \n",
    "            \n",
    "            message2 = message2.lower().split()\n",
    "            \n",
    "            #print(message)\n",
    "            p_spam_given_message = p_spam\n",
    "            p_ham_given_message = p_ham\n",
    "            \n",
    "            \n",
    "\n",
    "            for word in message2:\n",
    "               if word in parameters_spam:\n",
    "                  p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "               if word in parameters_ham: \n",
    "                  p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "            if p_ham_given_message > p_spam_given_message:\n",
    "               result = 'ham'\n",
    "            elif p_ham_given_message < p_spam_given_message:\n",
    "               result = 'spam'\n",
    "            else:\n",
    "                result = 'ham'\n",
    "               #result = 'Equal proabilities, have a human classify this!'\n",
    "    return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "229b69a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "1\n",
    "# TEST SINGLE INPUT\n",
    "\n",
    "#message = ('bonjour je veux un billet de paris à lyon')\n",
    "#messsage = ('tu as vu ce train à paris ?')\n",
    "#message = \"nous voudrions prendre l'avion pour Caen en partance de Paris\"\n",
    "#message = \"paris, lyon\"\n",
    "\n",
    "print(classify(message,p_word_given_ham['data'], p_word_given_spam['data'], parameters_spam['data'], parameters_ham['data'], p_spam['data'], p_ham['data']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770d5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
